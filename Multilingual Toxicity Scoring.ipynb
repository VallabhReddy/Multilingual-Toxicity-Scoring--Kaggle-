{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install -q nltk\n",
    "!pip install -q WordCloud\n",
    "!pip install -q plotly\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#NLP\n",
    "import string\n",
    "import re    #for regex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud  import WordCloud, STOPWORDS\n",
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Code Header\n",
    "- **Notebook Name**: Multilingual Toxicity Scoring\n",
    "- **Author(s)**: Vallabh Reddy\n",
    "- **Date**: 13th May 2020\n",
    "- **Edits to be made**:\n",
    "- **Additions Planned**:\n",
    "- **Workflow Plan**: \n",
    "    - Import data and preliminary setup\n",
    "    - Inspect datsets\n",
    "    - Visualize dataset properties\n",
    "    - Follow text preprocessing steps such as stemming, lemmatization, case generalization\n",
    "    - Wrangle the text datasets to extract unigrams, bigrams and trigrams ( Does \n",
    "    - Visualize the top used n-grams for toxicity and non-toxicity through word clouds and other means\n",
    "    - Investigate need for any other text representations required like tfidf, word vectors etc\n",
    "    - Pick models, train models. Should I train them only in English? Or would translating to other languages and then training models on that data help? Instead I could just translate test to english and then pass into model\n",
    "    - Use validation dataset to tune hyperparameters\n",
    "    - Test models on test dataset after translating\n",
    "    - Investigate value of ensembling\n",
    "- **Notes to Self**:\n",
    "    - How do I deal with spelling mistakes? Is there a way to coerce words to the right spelling using sentence context? Explore existing text analysis models for this.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "1. [VADER(Valence Aware Dictionary and sEntiment Reasoner)](https://pypi.org/project/vaderSentiment/) - The NLTK package contains the VADER tool which allows us to score the sentiment \n",
    "2. [HuggingFace's Transformers and Tokenizers](https://huggingface.co/transformers/) - HuggingFace has a collection of pretrained NLP models to pick from including Facebook's RoBERTa and Google's BERT. The same package comes with tokenizers to preprocess the text for these models\n",
    "3. [Jigsaw TPU: XLM-RoBERTa](https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta) ~ Xhlulu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [About This Project](#Problem-Space)\n",
    "- [About The Datasets](#about-datasets)\n",
    "- [Setup](#Setup)\n",
    "    - [Wrangling](#Wrangling)\n",
    "    - [Text Preprocessing](#Preprocessing)\n",
    "    - [EDA](#eda)\n",
    "- [someting](#third)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this project <a class=\"anchor\" id=\"Problem-Space\"></a>\n",
    "\n",
    "The Conversation AI team, a research initiative founded by Google and Jigsaw, is tasked with improving the vigilance against online toxicity in conversation. The goal of [this competition](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/overview) is to be able to predict the toxicity of multilingual comments using only English comments as our training data. \n",
    "\n",
    "Excerpts from the competition are given below.\n",
    "\n",
    ">It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet.\n",
    ">\n",
    ">In the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data.\n",
    ">\n",
    ">Jigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.\n",
    ">\n",
    ">As our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.\n",
    ">\n",
    ">*Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the datasets <a class = 'anchor' id = 'about-datasets'></a>\n",
    "We are given the following datasets.\n",
    "\n",
    "**Training set 1**\n",
    "- *Comment_text*: Contains the string that stores the comment.\n",
    "- *Toxic*: A boolean value, 1 = toxic, 0 = non-toxic.\n",
    "\n",
    "**Training set 2**: Has 'Comment_text' and 'toxic' similar to Training Set 1, but the 'Toxic' column is a probability. Also has several other descriptor probabilities.\n",
    "\n",
    "**Validation Set**\n",
    "- *Comment_text*: Same as Training Set 1.\n",
    "- *Toxic*: Same as Training Set 1.\n",
    "- *Lang*: Two letter representation of the language of the comment. 'es'= Espaniol, 'it' = Italian etc.\n",
    "\n",
    "**Test Set**\n",
    "- *Comment_text*: Same as Training Set 1.\n",
    "- *Lang*: same as Validation Set.\n",
    "- Does not have a 'Toxic' flag and we are tasked with predicting it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a class=\"anchor\" id=\"Setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the required datasets\n",
    "train_data_1 = pd.read_csv(\"Data/jigsaw-toxic-comment-train.csv\")\n",
    "train_data_2 = pd.read_csv(\"Data/jigsaw-unintended-bias-train.csv\")\n",
    "validation_data = pd.read_csv(\"Data/validation.csv\")\n",
    "test_data = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling <a class=\"anchor\" id=\"Wrangling\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the second dataset, the toxicity is not 1 or 0 but instead a probability, we will round it to convert to a 1/0 column\n",
    "train_data_2.toxic = train_data_2.toxic.round().astype(int)\n",
    "\n",
    "# We combined the entire training set 1 with all the toxic comments of training set 2 and 200k non-toxic comments from set 2\n",
    "train_data = pd.concat([\n",
    "                train_data_1[['comment_text','toxic']]\n",
    "                , train_data_2[['comment_text','toxic']].query('toxic == 1')\n",
    "                , train_data_2[['comment_text', 'toxic']].query('toxic == 0').sample(n = 200000, random_state = 1993)\n",
    "                ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(train_data.toxic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(validation_data.toxic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing <a class='anchor' id = 'Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(train_data.comment_text[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_vectorizer = CountVectorizer(stop_words = 'english', ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial = count_vectorizer.fit_transform(train_data.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the comment into single words\n",
    "text_words = word_tokenize(train_data.comment_text[1])\n",
    "# Converting to lower case\n",
    "text_words = [word.lower() for word in text_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_stopwords = stopwords.words('english')\n",
    "modified_stopwords.remove('not')\n",
    "#Removing stopwords and sumbols\n",
    "text_words = [word for  word in text_words if not word in modified_stopwords and word.isalpha()]\n",
    "len(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_data.sample(n = 10000, random_state = 1993)\n",
    "train_sample = train_sample.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA <a class = 'anchor'  id ='eda'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering comment text column, removing newline characters and filtering out unexpected data types from the column\n",
    "def nan_filter(x):\n",
    "    if type(x) == str:\n",
    "        return (x.replace(\"\\n\", \"\")).lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "nontoxic_text = ' '.join([nan_filter(comment) for comment in train_sample.query('toxic==0')['comment_text']])\n",
    "toxic_text = ' '.join([nan_filter(comment) for comment in train_sample.query('toxic == 1')['comment_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=300\n",
    "                      , background_color='white'\n",
    "                      , stopwords = modified_stopwords\n",
    "                      , collocations=True\n",
    "                      , max_words = 100\n",
    "                      , width=1200\n",
    "                      , height=1000).generate(nontoxic_text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "\n",
    "fig.update_layout(title_text='Non-Toxic Word Cloud(with bigrams)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=300\n",
    "                      , background_color='white'\n",
    "                      , stopwords = modified_stopwords\n",
    "                      , collocations=False\n",
    "                      , max_words = 100\n",
    "                      , width=1200\n",
    "                      , height=1000).generate(nontoxic_text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "\n",
    "fig.update_layout(title_text='Non-Toxic Word Cloud(unigrams)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=300\n",
    "                      , background_color='white'\n",
    "                      , stopwords = modified_stopwords\n",
    "                      , collocations=True\n",
    "                      , width=1200\n",
    "                      , max_words = 100\n",
    "                      , height=1000).generate(toxic_text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "\n",
    "fig.update_layout(title_text='Toxic Word Cloud(with bigrams)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=300\n",
    "                      , background_color='white'\n",
    "                      , stopwords = modified_stopwords\n",
    "                      , collocations=False\n",
    "                      , max_words = 100\n",
    "                      , width=1200\n",
    "                      , height=1000).generate(toxic_text)\n",
    "\n",
    "fig = px.imshow(wordcloud)\n",
    "\n",
    "fig.update_layout(title_text='Toxic Word Cloud(unigrams)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to note that in the toxic word cloud we see both 'hate' and 'like' as high frequency unigrams. On further inspection, I realized 'like' is used more often to compare the subject to something derogatory. \"You're acting like a ...\" , \" You're just like ..\" etc and less often in the sense \"I like ...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sample.iloc[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment size visualizations\n",
    "\n",
    "def text_len(x):\n",
    "    if type(x) is str:\n",
    "        return len(x.split())\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "train_sample['comment_size'] = train_sample.comment_text.apply(text_len)\n",
    "\n",
    "toxic_text_lengths = train_sample.query('toxic == 1 and comment_size < 200') ['comment_size'].sample(frac = 1, random_state = 1993)\n",
    "nontoxic_text_lengths = train_sample.query('toxic == 0 and comment_size < 200')['comment_size'].sample(frac = 1, random_state = 1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "ax = sns.distplot(toxic_text_lengths)\n",
    "plt.title('Toxic Comment Lengths')\n",
    "plt.xlabel('Comment Length')\n",
    "plt.xticks(np.arange(0,210,10))\n",
    "plt.yticks(np.arange(0,0.025,0.0025));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,5))\n",
    "ax = sns.distplot(nontoxic_text_lengths)\n",
    "plt.title('Non-Toxic Comment Lengths')\n",
    "plt.xlabel('Comment Length')\n",
    "plt.xticks(np.arange(0,210,10))\n",
    "plt.yticks(np.arange(0,0.025,0.0025));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Scores using VADER\n",
    "We'll try scoring the sentiment of the comments using the VADER component of NLTK. Here is an [article](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html) that expands on the procedure, it works better with social media content than general approaches. Here's a link to the original team's [paper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf).\n",
    "\n",
    "Note that the negative sentiment here is not the same as the toxicity we are looking for. Negativity might simply be portrayal of discontent, which is not toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(x):\n",
    "    if type(x) is str:\n",
    "        return SIA.polarity_scores(x)\n",
    "    else:\n",
    "        return 1000\n",
    "\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "train_sample['polarity'] = train_sample.comment_text.apply(sentiment)\n",
    "# Vader outputs 4 scores, Negative, Neutral, Positive and Compound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.query('toxic == 0').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_sample.query('toxic==1').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This comment has a negative score of 0 despite clearly being toxic.\n",
    "train_sample.comment_text[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On first look, it appears that VADER does not recognize negative terms when the writer masks characters with \\*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample['negativity'] = train_sample.polarity.apply(lambda x: x['neg'])\n",
    "train_sample['positivity'] = train_sample.polarity.apply(lambda x: x['pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Negativity Score with Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_negativity = train_sample.query('toxic == 0').sample(frac = 1, random_state = 1993)['negativity']\n",
    "toxic_negativity = train_sample.query('toxic == 1').sample(frac = 1, random_state = 1993)['negativity']\n",
    "\n",
    "plot = ff.create_distplot([nontoxic_negativity, toxic_negativity]\n",
    "                           , group_labels = ['Non-Toxic', 'Toxic']\n",
    "                           , colors = ['Green', 'Red']\n",
    "                           , show_hist= False)\n",
    "plot.update_layout(title_text = 'Negativity vs Toxicity'\n",
    "                   , xaxis_title = 'Negativity'\n",
    "                   , xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 0.1))\n",
    "\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negativity score seems to be able to differentiate the toxic comments from the non toxic to a certain extent. With a greater share of non-toxic comments having lower negativity and many toxic comments having at least a slight negative sentiment around 0.1-0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Positivity Score with Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontoxic_positivity = train_sample.query('toxic == 0').sample(frac = 1, random_state = 1993)['positivity']\n",
    "toxic_positivity = train_sample.query('toxic == 1').sample(frac = 1, random_state = 1993)['positivity']\n",
    "\n",
    "plot = ff.create_distplot([nontoxic_positivity, toxic_positivity]\n",
    "                          , group_labels=['Non-Toxic', 'Toxic']\n",
    "                          , colors = ['Green', 'Red']\n",
    "                          , show_hist= False)\n",
    "\n",
    "plot.update_layout( title_text = 'Positivity vs Toxicity'\n",
    "                    , xaxis_title = 'Positivity'\n",
    "                    , xaxis = dict(tickmode = 'linear', tick0 = 0, dtick = 0.1))\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we see that non-toxic comments have more observations at higher positivity levels, the positivity score does seems to be able differentiate toxic from non-toxic very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling <a class=anchor id='modelling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) XLM RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
